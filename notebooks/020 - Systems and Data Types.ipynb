{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "from IPython.lib import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems and Data Types\n",
    "\n",
    "As a reminder, here is Max Shron's definition of _data science_:\n",
    "\n",
    "> Data science is the application of math and computers to solve problems that stem from a lack of knowledge, constrained by the small number of people with any interest in the answers. --Max Shron, **Thinking with Data**\n",
    "\n",
    "He further identified how, when doing data science, we can get started by talking to people and having a CoNVO:\n",
    "\n",
    "1. Context\n",
    "2. Need\n",
    "3. Vision\n",
    "4. Outcome\n",
    "\n",
    "We do not start with data (or \"test tubes\").\n",
    "\n",
    "We also noted from Drew Conway's \"Venn Diagram of Data Science\" that Domain Knowledge was one of the inputs into data science. There is actually some disagreement here. The \"anti\" domain knowledge camp asserts that doing data science involves taking a data set and applying a portfolio of techniques to it but you do not need to have any domain knowledge. Math is math. \n",
    "\n",
    "The \"pro\" domain knowledge camp asserts that domain knowledge is required. I fall squarely into the second camp. I find that those in the first camp tend to be very machine learning oriented and, not surprisingly, very pro-Kaggle type competitions. For example, one member of \"anti\" camp has won multiple such machine learning competitions.\n",
    "\n",
    "The problem with this position is that a Kaggle competition starts out with a selected data set and problem to solve. All of the hard work is already done. The problem has been identified and the data obtained. Additionally, there have been some disastrous blog posts by people just \"grabbing\" data and trying to out analyze domain subject experts on Social Security and Climate Change.\n",
    "\n",
    "In my experience, the first weeks and sometimes months of a new job or orienting a new team member involves figuring out the domain vocabulary, data locations and availability and basic goals of the organization. In other words, you need to get domain knowledge. But this presents a problem for teaching data science because I cannot teach domain knowledge.\n",
    "\n",
    "I can, however, describe one technique for thinking about the domain or at least a way of communicating with whomever you are working with (even if it's just yourself!) so that everyone has a shared mental model. And, as it turns out, this technique is also a good basis for talking about what data is and where it comes from. The technique is Systems Thinking (more specifically *System Dynamics*). You can apply the principles I'll describe whether you're working by yourself or with a team.\n",
    "\n",
    "## Systems Thinking\n",
    "\n",
    "Humans aren't particulary good at systems. We can generally follow through a few steps of cause and effect but it gets very difficult to take everything into account and to actually predict how the system will behave. When doing data science, we almost always want the end result to be for someone to *do* something. But we cannot be simplistic about our predictions and the affects of our actions. The very act of personalizing a website may change to what degree personalization works. We cannot assume our competitors will sit idlely by. And even if we cannot *model* all of this complexity, we should at least be thinking about it through the process from developing a model to deploying an intervention.\n",
    "\n",
    "![Karma](https://almostcandidtv.files.wordpress.com/2014/07/karma-comic.jpg)\n",
    "\n",
    "[System Dynamics](https://en.wikipedia.org/wiki/System_dynamics) was developed by Jay Forrester to model (complex) systems. The basic approach is to identify key variables of the system and their influences and model them with a set of differential equations. More specifically we look at *levels* (population, industrial capacity, atmospheric carbon), *rates of change* (birth and death rates, innovation rates, production rates) and parameters. These can all be captures in terms of System Dynamics' Stock and Flow Models. Using a variety of software packages, these diagrams can actually be executed and the behavior of the system predicted in a qualitative way.\n",
    "\n",
    "The most famous application of System Dynamics was the Club of Rome's [Limits to Growth](https://en.wikipedia.org/wiki/The_Limits_to_Growth) report published in 1972. However, it was originally envisioned as a technique for helping management as Forrester was actually in MIT's Sloan School of Management. The first use of System Dynamics (all computations by hand!) was probably Forrester's consulting work with GM to explain a 3 year cycle in employment at a Kentucky GM plant.\n",
    "\n",
    "While Stock and Flow Models are interesting, I'm mostly interested in the more qualitative aspects of System Dynamics as a back drop for data science. For example, if you're trying to investigate one of the Kaggle data science use cases, say, training recommendations in an HR context or monitoring contractor performance in construction, then we want to be able to quickly sketch out the main relationships of the context, need and vision.\n",
    "\n",
    "For this purpose, I think that System Dynamics' Causal Loop Diagrams are very useful. They can be used during the early stages of context and need identification with domain and subject matter experts, stakeholders and other analysts. They can improve communication, establish a shared mental model of the system being evaluated or manipulated, and bring to light unknown factors and hidden assumptions. This is true even if it's just you doing the analysis yourself.\n",
    "\n",
    "But first let's start at the beginning. What is a system?\n",
    "\n",
    "### What is a System?\n",
    "\n",
    "What is a [System](https://en.wikipedia.org/wiki/System)? Good question. At this point we simply defer to Wikipedia:\n",
    "\n",
    "> Some systems share common characteristics, including:\n",
    "\n",
    "> * A system has structure, it contains parts (or components) that are directly or indirectly related to each other;\n",
    "* A system has behavior, it exhibits processes that fulfill its function or purpose;\n",
    "* A system has interconnectivity: the parts and processes are connected by structural and/or behavioral relationships;\n",
    "* A system's structure and behavior may be decomposed via subsystems and sub-processes to elementary parts and process steps;\n",
    "* A system has behavior that, in relativity to its surroundings, may be categorized as both fast and strong.\n",
    "\n",
    "> The term system may also refer to a set of rules that governs structure and/or behavior. Alternatively, and usually in the context of complex social systems, the term institution is used to describe the set of rules that govern structure and/or behavior.\n",
    "\n",
    "Systems are often part of larger systems (and systems often have smaller parts that are themselves systems). A cell is a system. The digestion is handled by a system (even though it includes cells). A person is a system (even though it has a digestive system). A family is a system (even though it includes persons). And so on.\n",
    "\n",
    "Where you draw the line depends on what level of behavior you're interested in.\n",
    "\n",
    "When talking about systems it's convenient to have a notation for describing the system and that's where Causal Loop Diagrams come in. For more information on Systems Thinking try [Systems & Us](http://systemsandus.com/) which approaches some interesting problems from the viewpoint of System Dynamics (although they don't *always* get it right, the assumptions are spelled out).\n",
    "\n",
    "### Causal Loop Diagram\n",
    "\n",
    "A Causal Loop Diagram (CLD) is a directed graph where the nodes are features and the directed edges describe which features are influenced by a feature as causal links. Each causal link has a *polarity* and can be positive or negative which is denoted by a plus or minus sign by the arrow head. The polarity indicates what happens to one variable when another variable changes. \"plus\" means they move in the same direction and \"minus\" means they move int the opposite direction. Hashes on a link indicate a temporal delay of some kind in the causality.\n",
    "\n",
    "The main characteristic of a CLD are causal or feedback loops which can be reinforcing or balancing. Consider three features: births, population and deaths. The causal link between births and population is positive as is the causal link between population and births. Therefore the loop from births to population to births is a positive, reinforcing loop. Reinforcing loops have an *even* number of negative edges (zero is counted as even).\n",
    "\n",
    "The link from population to deaths is also positive. The link from deaths to population, however, is negative. Therefore the loop from population to deaths to population is balancing. Balancing loops have an *odd* number of negative edges. [Reading Causal Loop Diagrams](http://systemsandus.com/2012/08/15/learn-to-read-clds/).\n",
    "\n",
    "![Births and Deaths](https://systemsandus.files.wordpress.com/2012/07/reinforcing-and-balancing-loops-and-delays.jpg)\n",
    "\n",
    "After starting simple, we can add more detail to the model. What roles do life expectancy, resources and family planning have on our model?\n",
    "\n",
    "![Additional System Features](https://systemsandus.files.wordpress.com/2012/07/two-more-loops.jpg)\n",
    "\n",
    "My suggestion is that if you are starting with the problem (either alone or on a team), start creating a causal loop diagram so that you can all clarify your mental model of the problem, the needs, vision and outcome.\n",
    "\n",
    "If you are starting with the data, go back and try to build the CLD that gives rise to the data you have even if it means adding features that you *do not* have have data for. This will clarify you're thinking about the data you have and the relationships you expect to see.\n",
    "\n",
    "There's a lot more to System Dynamics than this. For example, there are *archetypal* patterns in complex systems. Additionally, Donella Meadows identified as the [12 leverage points](https://en.wikipedia.org/wiki/Twelve_leverage_points) in a system. My personal view is that System Dynamics contains many of the most useful insights of economics and more (for example, decreasing returns). MIT still teaches a [Systems Dynamics](http://mitsloan.mit.edu/group/system-dynamics/) course.\n",
    "\n",
    "Here are some resources to help you with Causal Loop Diagrams. They all have their pluses and minuses, you don't need to watch all of them...listen to the first 5-10 minutes and see what one appeals to you. Still, if you do watch all of them you'll get a more complete view of Causal Loop Diagrams. \n",
    "\n",
    "The first is a pretty good overview of why you want to use CLDs but the accent is sometimes distracting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/WDqVZ9VV4Kg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1067da450>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo( 'WDqVZ9VV4Kg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also a good account of CLDs and one is developed but the sound is bad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/EjC6xv9PT9Y\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1067da850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('EjC6xv9PT9Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fairly extensive discussion of 5 Causal Loop Diagrams is presented here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/pEJ_YNHeRuc\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1067da610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('pEJ_YNHeRuc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also grab a set of slides from here:\n",
    "\n",
    "[Causal Loop Diagrams](https://obssr.od.nih.gov/issh/2009/files/ISSH2009_SDTrack_Lecture3_Causal_Loop_Diagrams_Structure_Behavior.pdf)\n",
    "\n",
    "That I think bring up some relevant modeling issues that we were unable to discuss or address in lecture.\n",
    "\n",
    "There is free software for drawing Causal Loop Diagrams (the software will do Stock and Flow Models as well). This is the software used in two of the videos above:\n",
    "\n",
    "[Vensim]( http://vensim.com/free-download/)\n",
    "\n",
    "It is free for personal use.\n",
    "\n",
    "If you want more examples of CLDs, just google \"Causal Loop Diagrams\" and click on \"Images\". There are tons of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "So what does this have to do with data?\n",
    "\n",
    "Well, data is a collection of observations of a system, specifically, the system relevant to your context and need. If it isn't, you haven't described the underlying system very well. The data will be like snapshots of that system at a point in time or different points in time. If you work the other way (you started with your data), then the system you describe provides context for you data and, as we'll see later, your exploration of the data.\n",
    "\n",
    "Having both the data and a description of the system, you can start asking yourself questions:\n",
    "\n",
    "1. How does the system you've described give rise to the data that you've observed?\n",
    "2. How does the data that you've observed relate to the system you've described? What is missing?\n",
    "\n",
    "As a concrete example, imagine we started with the Population CLD above. If we want to estimate the parameters of that model, we need to find data. As it turns out that kind of data is available in the decennial US Census. The Census data is a collection of observations with each row representing values for various features including: year, name, address, parents, income, age, country of origin, number of years at the residence, etc.\n",
    "\n",
    "Suppose we started out with the Census data. Then we might arrive at the Population CLD above as a description of the system that gives rise to our data as well as context.\n",
    "\n",
    "And it also allows us to check our model. For example, if we were looking at just a single city in Iowa, then the Population CLD above might prove to be a poor model. If we start looking at the Census data and assume that anyone recorded at one census and not recorded in a subsequent one has died, we'll get an incorrect estimate of the death rate because our mental model is wrong or incomplete. The problem is that we're missing migration.\n",
    "\n",
    "### Data Types\n",
    "\n",
    "So we have a system and variables. Those variables can be of different types, different kinds of measurements and assignments. Here is one break down of data types:\n",
    "\n",
    "- Qualitative\n",
    "\t* **Nominal** - the characteristic is a label (Republican, Independent, Democrat), (North America, South America, Africa, Europe, Asia, Australia) and has no order.\n",
    "\t* **Ordinal** - the characteristic is a label but has an order: 1st, 2nd, 3rd, 4th, etc. 4th - 2nd makes no sense, Likert (1-5, 0-10) scales.\n",
    "- Quantitative\n",
    "\t* **Interval** - an ordered numeric space of intervals where 4 - 2 makes sense. Fahrenheit, for example, divides the space between 0 (freezing) and 212 (boiling) into equal intervals. 0 does not mean \"no value\".\n",
    "\t* **Ratio** - 0 means \"no value\". Height, weight, Kelvin scale. Very often for suchd data, there are no negative values either (although negative derived values are possible).\n",
    "\n",
    "Here's another, more detailed one from Wikipedia for [Statistical Data Types](https://en.wikipedia.org/wiki/Statistical_data_type).\n",
    "\n",
    "- Simple Types\n",
    "\t* **binary** - nominal (yes, no) also called dichotomous.\n",
    "\t* **categorical** (nominal) - assignments to groups.\n",
    "\t* **ordinal** (ordinal) - labels indicating ordering.\n",
    "\t* **count** (ratio) - number of items in an interval/area/volume.\n",
    "\t* **real-values**\n",
    "\t\t+ additive - temperature (20 degrees - 13 degrees makes sensem, 0 doesn't mean \"none\")\n",
    "\t\t+ multiplicative - weight (0 means \"none\")\n",
    "- Complex Types\n",
    "\t* **Money** (amount and currency)\n",
    "\t* **Date** (ordinal but different schemes)\n",
    "\t* **Time** (ordinal and ratio and different schemes)\n",
    "\t* **Location** (latitude and longitude)\n",
    "\t* **Relational** (friends)\n",
    "\n",
    "You don't need to know about the other columns in the table until later in the semester (distribution, statistics, etc.).\n",
    "\n",
    "The two different kinds of real values can be confusing but essentially for the additive real valued variable you can do addition but multiplication doesn't really apply. It is related to the 0 not meaning \"none\". For example, 0 degrees Fahrenheit is some temperature and 2 $\\times$ 0 should mean twice as warm but it's still 0. Whereas with Kelvin (a multiplicative scale), 2 $\\times$ 0 is still 0 and that makes sense because 0 *is* \"none\".\n",
    "\n",
    "Additionally, quantitative measurements can be continuous or discrete. For example, counts are generally discrete whereas general measurements of distance, height, weight, etc., are at least theoretically continuous down to the resolution of the measuring device.\n",
    "\n",
    "Probably the most important take away from this is that not all numbers can have arithmetic operations performed on them. For example, average *place* is not really anything so despite the fact that your statistics package might identify the data as numeric, that doesn't mean that addition and subtraction apply. Some people assert the same is true of Likert type scales (1 = not important at all to 5 = very important). Because they are relative and cannot be calibrated between people (is my somewhat important the same as yours?), taking and average of Likert values is not appropriate. Another problem area is codes or encodings. These will show up as numbers but are really categorical *codes*.\n",
    "\n",
    "We can summarize the problem by saying not all numbers are quantitative.\n",
    "\n",
    "#### Transformations\n",
    "\n",
    "As we will soon see, it is often handy to be able to convert between continuous and discrete, qualitative and quantitative values. We sometimes want to convert between different types of qualitative and quantitative values as well or derive new measurements.\n",
    "\n",
    "An example of the first type of transformation is converting income (a real valued multiplicative quantitative measurement) into income ranges (an ordinal qualitative measurement). An example of the second type of transformation is converting arrival time (a qualitative ordinal measurment) into *inter* arrival time (a quantitative real-valued multiplicative measurement).\n",
    "\n",
    "*Qualitative to Quantitative Transformations*\n",
    "\n",
    "* Binary to Integer\n",
    "\n",
    "The most basic transformation is the transformation of binary/dichotomous data from yes/no, success/failure, alive/dead to 0 or 1. Many other transformations are based on this basic transformation.\n",
    "\n",
    "* Categorical to Binary\n",
    "\n",
    "Most statistical and machine learning techniques work with numbers not symbols. We therefore often need to convert symbols to number somehow. We often do this by counting. For example, to summarize a \"religion\" data, we will count the number of Christians, Jews, Muslims, Buddhists, Atheists, etc. and report the absolute or relative counts (proportions) for each label. This does not, however, help us with any algorithm that works only on numerical values.\n",
    "\n",
    "We cannot simply assign numbers to our categories. For example, Christian = 0, Jewish = 1, Muslim = 2, Buddhist = 3, Atheist = 4, etc., is not any better/different/definitive than Christian = 4, Jewish = 2, Muslim = 1, Buddhist = 0, Atheist = 3, etc. Not only does Jewish - Buddhist make no sense whatsoever but 2 * Jewish != Christian!\n",
    "\n",
    "The general solution to this problem is to use \"One Hot Encodings\". Given a categorical feature of m values, we create m binary features with two values named after each possible value where 0 indicates \"not it\" and 1 indicates \"it\". Thus an observation with Religion = Buddhist would become Christian = No, Jewish = No, Muslim = No, Buddhist = Yes, Atheist = No.\n",
    "\n",
    "*Quantitative to Qualitative Transformations*\n",
    "\n",
    "Some algorithms and summaries work better for real-valued measurements if they are converted to categorical (ordinal) values. For example, nobody wants to look at a distribution of ages by year with 100+ possible values. The processing of converting real-valued measurements into categorical ones is often called discretization or binning. Oddly, discretization does not actually turn the values into discrete *numeric* values but categorical, ordinal ones. We will use *binning* here. We will often want to bin a continuous feature to improve exploratory data analysis (EDA) or improve statistical analysis/machine learning. We will have more to say about each when the time comes.\n",
    "\n",
    "Sometimes there are natural, socially or culturally important bins. For example, ages are often binned up to 16 or 18, then to 21, then by decade to 64, then 65 plus. For EDA type binning, there are large number of algorithms for binning including:\n",
    "\n",
    "* square-root choice\n",
    "* Sturges's formula\n",
    "* Rice rule\n",
    "* Doane's formula\n",
    "* Scott's normal reference rule\n",
    "* Freedman-Diaconis rule\n",
    "* Minimized L2 risk\n",
    "\n",
    "When we get to Exploratory Data Analysis and histograms, we'll talk more about this. The bins used in machine learning are often generated by more sophisticated algorithms using entropy or mutual information.\n",
    "\n",
    "The interesting thing about data types is that you can derive completely different data types from other data types. For example, if you have data about runners and their \"place\" in a race, that's *ordinal* data but differences in place are real-valued, ratio data because 0 means \"no difference\" (ties have the same \"place\"). Additionally, the difference can be negative or positive.\n",
    "\n",
    "Timestamped data is another good example. You might have data that shows when a customer first landed on your website. As it is, the timestamp is just ordinal data. But you can transform this data into number of people per hour which is counts or differences in time (after converting to seconds since the Epoch) which is ratio data.\n",
    "\n",
    "There are other transformations you can do to create new features and we'll talk about them when we talk about modeling.\n",
    "\n",
    "#### Granularity\n",
    "\n",
    "Another thing you need to be aware of when working with data is the *granularity* of the data relative to your CoNVO and other data you're using. To illustrate the potential problem of not considering granularity let's consider to different data science problems. The first involves comparing government budgeting (fiscal policy) at the state level The second involves looking at crime at the state level.\n",
    "\n",
    "In the first case, states are properly the unit of observation for comparing state fiscal budgets. Budgets happen *to* states. In the second case, however, crime really doesn't happen *to* states. Crimes happen to individuals and crime statistics can be aggregated to the state level.\n",
    "\n",
    "Thinking about granularity becomes even more important when you try to combine data sets. For example, you might have individual data about your customers but you lack information about their income and education levels and you think they are relevant to your problem. You might be able to *enrich* your primary data with income and education data if you can obtain it.\n",
    "\n",
    "One approach would be to simply ask your customers but this might not go over well. Another approach would be to obtain the data from private or public sources. These other sources might not have individual data but instead have data based on Census Tract, Zip or Postal Code, City/County or even State data. If you use this data, however, there will be a mismatch in granularity between the data sources.\n",
    "\n",
    "What this all boils down to is being aware of the unit of observation relevant to your problem (context and need) and whether the data you have or data you obtain is at the same level. It doesn't *have* to be and sometimes it simply cannot be but you have to acknowledge that the aggregation imposes limits on the conclusions you can draw or may affect the model.\n",
    "\n",
    "For example, crime statistics could vary between states simply because of different levels of urbanization (think New Jersy v. Wyoming). Or if you enrich your customer data with City-level income estimates then everyone in that city gets the same estimate. If most of your customers are from that one city, the income information doesn't add anything. On the other hand, zip code level income estimates might be more useful.\n",
    "\n",
    "In the last section we talked about transforming web sites into counts per hour. This transforms the granularity of the data. For example, turning any set of observations into counts will affect the level of granularity.\n",
    "\n",
    "#### Structured and Unstructured Data\n",
    "\n",
    "We tend to think of data as a rectangular array where the rows are observations from the system of interest and columns are the features. Every cell is a value. Most analysis and modeling is geared towards data of this type. This is generally referred to as *structured* data.\n",
    "\n",
    "There are a lot of features, however, that are not nice neat values (we already talked about complex data types earlier). The more typical examples are text (blog posts, comments, books, etc), images (GIFs, JPEGs, PNGs, analog paintings, etc.), videos (YouTube videos, movies, TV shows, and their analog versions), and sounds (music, etc.). If you want to get an idea of where they might arise, try to think of a system described by a Causal Loop Diagram where a blog post, a comment or a product review is one of the variables. How do we work with that?\n",
    "\n",
    "There are a variety of techniques associated with turning these *unstructured* data sources into structured ones. We will concentrate on images and text.\n",
    "\n",
    "*Images*\n",
    "\n",
    "Images are, to a certain extent, easy because (for at least the digital ones) there are already structured representations. An image appears as a matrix of pixels but computer memory is linear so that matrix is really an array of pixels, one row after the other. We can use the same representation.\n",
    "\n",
    "Additionally, the pixel color can be decomposed into one of several color models (for example, RGBA) so that the array can be duplicated 4 times with the first array representing the red pixel values, the second array representing the green pixel values, the third array representing blue and the final array representing the alpha value. Of course, an image that is 720x540 decomposed into 4 features has a total of 1,555,200 possible values. You can't easily integrate that into other data. Instead, you might need to use some transformation or clustering technique to make the data size manageable.\n",
    "\n",
    "*Text*\n",
    "\n",
    "Text is much more difficult to work with because it doesn't appear to have any obvious way to turn it into numbers. Specific words follow after words and specific sentences follow after specific sentences or it's just all so much gibberish. Or is it?\n",
    "\n",
    "It turns out that for many applications it's sufficient to treat a document, even something as small as a tweet, as a \"bag of words\". The bag of words model entails treating each document as if the words had no order and either simply noting their presence in the document or counting how many times they appear in the document. If we do this for each document, we will end up with structured data where each row represents a document, each column represents a particular word and each cell is some numeric value.\n",
    "\n",
    "There are, however, a few questions that arise when you actually try to go and do this such as:\n",
    "\n",
    "0. What is a word?\n",
    "1. What about punctuation?\n",
    "2. What about capitalization? If \"The\" and \"the\" appear are they the same or different?\n",
    "3. And What about \"the\" and \"and\" and all the rest of the very common words? Do we really need to count them?\n",
    "4. What about related words? inspection, inspected, inspector, inspecting? Are these to be counted as the same or different?\n",
    "5. What about declension? good, better, best? bad, worse, worst? am, are, was?\n",
    "\n",
    "The answers to each question depend (as always) on the task at hand. The first two problems (and the solution) involves [tokenization](https://en.wikipedia.org/wiki/Tokenization_(lexical_analysis)). Tokenization can occur at the glyph, word, or sentence level according to the problem. If the data was acquired via an API or web scraping, this might also include removing HTML or XML markup. Removing punctuation which used to be a no-brainer is not entirely clear in the Age of Emojis. A \"wink\" emoji can turn a comment from mean to sarcastic. And a \"frowny\" emoji can change the entire emotion behind a review. This makes the tokenization task more difficult. So simply removing emojis can remove important semantic information.\n",
    "\n",
    "The next problem refers to regularization. If capitalization are not important for the particular application, then you downcase everything and remove capitalization. \n",
    "Removing common words is almost always a good idea. These are called \"stop words\" and there are lists of stop words for various languages. Transforming words to their common root is accomplished by using [stemming](https://en.wikipedia.org/wiki/Stemming) which follows rules for reducing English (or the appropriate language) to their stems. The transformations that result from grammatical rules (intensifiers, verb tense declension) are \"undone\" using [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation). This isn't often done for simple applications.\n",
    "\n",
    "The general approach is called [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Python has the raw tools available to accomplish most of these tasks. We can write regular expressions to identify the tokens of interest. Using a list of stop words, we can remove them from the document. A stemmer is also available.\n",
    "\n",
    "There are two Python libraries that have modules for dealing with text, however. [Natural Language Toolkit](http://www.nltk.org/) and [Scikit Learn](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html). Although the documentation for either is not as straight forward as one would like.\n",
    "\n",
    "Once you have reduced a document to a bag of words, there are a number of ways that it can be translated into a numeric feature vector. We might simply be interested in whether or not the word occurs in the document. In this case, we create a binary feature for each token (tokens are more general than words). We might be interested in the absolute counts--how many times each token appears in the document--but because larger documents have more words in general, we probably want to normalized token counts to *relative* frequency.\n",
    "\n",
    "When we talk about modeling, we'll discuss further adjustments that are required to correctly use text data in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
